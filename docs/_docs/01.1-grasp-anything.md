---
title: "Grasp-Anything"
permalink: /docs/grasp-anything/
excerpt: "Overview of Grasp-Anything."
redirect_from:
  - /theme-setup/
toc: true
---

Grasp-Anything dataset is synthesized from foundation models. With **1 million** samples and **3 million** diverse objects, it significantly surpasses previous datasets, facilitating zero-shot learning in both simulated and real-world settings.

## Data Pipeline
<p align="center">
  <img src="../../assets/images/generation_pipeline.png" alt="data_pipeline" style="width: 100%;" />
</p>


We use ChatGPT to generate an expansive array of scene descriptions. Next, we transform these scene descriptions into images using Stable-Diffusion 2.1 [[1]](#references), and label them with a pretrained RAGT-3/3 model. These grasping poses are post-proceeded to ensure high quality.

## Demonstration
<video width="100%" controls>
  <source src="../../assets/images/ICRA24_0520_VI_fi_compressed.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## Samples
<p align="center">
  <img src="../../assets/images/grasp-anything-samples.png" alt="grasp-anything-sample" style="width: 100%;" />
</p>

## References
<a name="references"></a>
- [1] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2020.

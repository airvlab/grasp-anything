---
title: "Grasp-Anything++"
permalink: /docs/grasp-anything-pp/
excerpt: "Overview of Grasp-Anything++."
redirect_from:
  - /theme-setup/
toc: true
---

Grasp-Anything++ [[1]](#references) extend over Grasp-Anything and includes **10 million** grasping instructions and associated ground truth. 

Our dataset can be used for **language-driven grasping** task and allows the robots to grasp specific objects based on language commands.

## Samples
<p align="center">
  <img src="../../assets/images/grasp-anything-pp-samples.png" alt="samples" style="width: 100%;" />
</p>

## Language-driven Grasping
<p align="center">
  <img src="../../assets/images/grasp-anything-pp-methodology.png" alt="method" style="width: 100%;" />
</p>

We introduce a new language-driven grasping method based on conditional-guided diffusion models [[2]](#references) with a new contrastive training objective.

## References
<a name="references"></a>

- [1] [An Dinh Vuong](https://andvg3.github.io/), Minh Nhat Vu, Baoru Huang, Nghia Nguyen, Hieu Le, Thieu Vo, [Anh Nguyen](https://www.csc.liv.ac.uk/~anguyen/). *Language-driven Grasp Detection*. In CVPR, 2024.

- [2] [An Dinh Vuong](https://andvg3.github.io/), Minh Nhat Vu, Toan Tien Nguyen, Baoru Huang, Dzung Nguyen, Thieu Vo, [Anh Nguyen](https://www.csc.liv.ac.uk/~anguyen/). *Language-driven Scene Synthesis using Multi-conditional Diffusion Model*. In NeurIPS, 2023.

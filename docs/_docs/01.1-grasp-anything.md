---
title: "Grasp-Anything"
permalink: /docs/grasp-anything/
excerpt: "Overview of Grasp-Anything."
redirect_from:
  - /theme-setup/
toc: true
---

Grasp-Anything introduces a new dataset synthesized from foundation models, targeting the challenge of grasp detection in robotics. With **1 million** samples and **3 million** diverse objects, it significantly surpasses previous datasets in *diversity*, facilitating advanced zero-shot and language-driven grasp detection in both simulated and real-world settings.

## Data Pipeline

[generation_pipeline](https://github.com/airvlab/grasp-anything/assets/140178004/6a66c0e7-2172-45fb-a373-e659a58cfd05)

Our data pipeline kicks off by tasking ChatGPT with generating an expansive array of scene descriptions in a specific format, aiming for a diverse set of scenarios that include a broad spectrum of objects. Next, we transform these scene descriptions into images using Stable-Diffusion 2.1 [1], and then, with a pretrained RAGT-3/3 model [2], we create plausible grasp poses. These poses are assessed for their viability based on established theories, making our dataset not only vast but of high quality.

## Demonstration
<video width="100%" controls>
  <source src="https://github.com/airvlab/grasp-anything/assets/140178004/7afc471e-385d-4aff-9940-a87fc3fe034e" type="video/mp4">
  Your browser does not support the video tag.
</video>

## Samples
Here are some samples from the Grasp-Anything dataset.
![grasp_visualization-Page-1](https://github.com/airvlab/grasp-anything/assets/140178004/e40063c4-623e-4663-a7e7-f4c00c5fe0d2)

## References
[1] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).

[2] Cao, B., Zhou, X., Guo, C., Zhang, B., Liu, Y., & Tan, Q. (2023). NBMOD: Find It and Grasp It in Noisy Background. arXiv preprint arXiv:2306.10265.


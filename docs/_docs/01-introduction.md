---
title: "Introduction"
permalink: /docs/introduction/
excerpt: "Large-scale dataset for grasp detection."
redirect_from:
  - /theme-setup/
toc: true
---
Welcome to Grasp-Anything project! We tackle grasp detection by utilizing foundation models. Our project represents a <strong>data centric</strong> approach for grasp detection.

## Dataset Comparison
<p align="center">
  <img src="../../assets/images/intro-dataset-comparison.png" alt="dataset_comparison" style="width: 100%;" />
</p>

Grasp-Anything offers *universality*, featuring a wide range of everyday objects in natural arrangements, unlike other benchmarks limited by object selection and controlled settings.

## Statistics

<p align="center">
  <img src="../../assets/images/pos-tags.png" alt="pos-tags" style="width: 50%;" />
  <img src="../../assets/images/num_cats.png" alt="num_cats" style="width: 48%;" />
</p>

To categorize words in a text according to their grammatical roles and syntactic functions, we extract the POS tags in our dataset and visualize them in the above figure. Our scene descriptions corpus utilizes a wide range of words to describe scene arrangements. In addition, we also compare object shape distributions between Grasp-Anything and Jacquard. The outcome implies that objects in Grasp-Anything span over a greater area than Jacquard's, indicating a greater degree of shape diversity.

<p align="center">
  <img src="../../assets/images/shape_visualization.png" alt="shape_visualization" style="width: 80%;" />
</p>


## Stay Updated

This website will be continuously updated with the latest papers, datasets, and code. We encourage you to check back regularly for updates.



## References
[//]: # [1] Gupta, A., Dollar, P., & Girshick, R. (2019). Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 5356-5364).

[//]: # [2] Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L. H., ... & Gao, J. (2022). Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16793-16803).

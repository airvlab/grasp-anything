---
title: "Grasp-Anything++"
permalink: /docs/grasp-anything-pp/
excerpt: "Overview of Grasp-Anything++."
redirect_from:
  - /theme-setup/
toc: true
---

Grasp-Anything++ [[1]](#references) extend over Grasp-Anything and includes **10 million** grasping instructions and associated ground truth. 

Our dataset can be used for **language-driven grasping** task and allows the robots to grasp specific objects based on language commands.

## Samples
<p align="center">
  <img src="../../assets/images/grasp-anything-pp-samples.png" alt="samples" style="width: 100%;" />
</p>

## Language-driven Grasping
<p align="center">
  <img src="../../assets/images/grasp-anything-pp-methodology.png" alt="method" style="width: 100%;" />
</p>

We introduce a new language-driven grasping method based on conditional-guided diffusion models [[2]](#references) with a new contrastive training objective.

## References
<a name="references"></a>

- [1] Vuong, A. D., Vu, M. N., Huang, B., N. Nguyen, H. Le, Vo, T., & Nguyen, A. Language-driven Scene Synthesis using Multi-conditional Diffusion Model. In CVPR, 2024.

- [2] Vuong, A. D., Vu, M. N., Nguyen, T., Huang, B., Nguyen, D., Vo, T., & Nguyen, A. Language-driven Scene Synthesis using Multi-conditional Diffusion Model. In NeurIPS, 2023.

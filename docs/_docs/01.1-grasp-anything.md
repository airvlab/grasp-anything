---
title: "Grasp-Anything"
permalink: /docs/grasp-anything/
excerpt: "Overview of Grasp-Anything."
redirect_from:
  - /theme-setup/
toc: true
---

Grasp-Anything [[1]](#references) dataset is synthesized from foundation models. With **1 million** samples and **3 million** diverse objects, it significantly surpasses previous datasets, facilitating zero-shot learning in both simulated and real-world settings.

## Data Pipeline
<p align="center">
  <img src="../../assets/images/generation_pipeline.png" alt="data_pipeline" style="width: 100%;" />
</p>


We use ChatGPT to generate an expansive array of scene descriptions. Next, we transform these scene descriptions into images using Stable-Diffusion [[2]](#references), and label them with a pretrained RAGT-3/3 model. These grasping poses are post-proceeded to ensure high quality.

## Demonstration
<video width="100%" controls>
  <source src="../../assets/images/ICRA24_0520_VI_fi_compressed.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## Samples
<p align="center">
  <img src="../../assets/images/grasp-anything-samples.png" alt="grasp-anything-sample" style="width: 100%;" />
</p>

## References
<a name="references"></a>

- [1] An Dinh Vuong, Minh Nhat Vu, Hieu Le, Baoru Huang, Binh Huynh, Thieu Vo, Andreas Kugi, Anh Nguyen. Grasp-anything: Large-scale grasp dataset from foundation models. In ICRA, 2024.

- [2] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Omme. High-resolution image synthesis with latent diffusion models. In CVPR, 2020.
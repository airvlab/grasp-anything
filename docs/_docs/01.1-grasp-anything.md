---
title: "Grasp-Anything"
permalink: /docs/grasp-anything/
excerpt: "Overview of Grasp-Anything."
redirect_from:
  - /theme-setup/
toc: true
---

Grasp-Anything dataset is synthesized from foundation models. With **1 million** samples and **3 million** diverse objects, it significantly surpasses previous datasets in *diversity*, facilitating advanced zero-shot in both simulated and real-world settings.

## Data Pipeline
<p align="center">
  <img src="../../assets/images/generation_pipeline.png" alt="data_pipeline" style="width: 100%;" />
</p>


We use ChatGPT to generate an expansive array of scene descriptions. Next, we transform these scene descriptions into images using Stable-Diffusion 2.1 [1], and label them with a pretrained RAGT-3/3 model [2]. These grasping poses are post-proceeded to ensure high quality.

## Demonstration
<video width="100%" controls>
  <source src="../../assets/images/ICRA24_0520_VI_fi_compressed.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## Samples
<p align="center">
  <img src="../../assets/images/grasp-anything-samples.png" alt="grasp-anything-sample" style="width: 100%;" />
</p>

## References
[1] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).

[2] Cao, B., Zhou, X., Guo, C., Zhang, B., Liu, Y., & Tan, Q. (2023). NBMOD: Find It and Grasp It in Noisy Background. arXiv preprint arXiv:2306.10265.


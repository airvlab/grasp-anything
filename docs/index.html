<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Grasp-Anything: Large-scale Grasp Dataset from Foundation Models">
  <meta name="keywords" content="visual navigation,embodied AI,human dynamics model, deep reinforcement learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="./static/images/grasp-anything.png"/>
  <link rel="image_src" href="./static/images/grasp-anything.png">
  <link rel="icon"
        type="image/x-icon"
        href="./static/images/grasp-anything.ico"/>

  <title>Grasp-Anything: Large-scale Grasp Dataset from Foundation Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EDF010G6PN"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EDF010G6PN');


  </script>

  <script type="module"
          src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
  <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="./static/slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css"/>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="container has-text-centered">
        <h1 class="title is-1 publication-title">
          Grasp-Anything: Large-scale Grasp Dataset from Foundation Models
        </h1>
		<h2 class="is-size-5 publication-authors">
			ICRA 2024
		</h2>
        <!-- <br> -->
        <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=CUpnG-YAAAAJ&hl=en&oi=ao">An Dinh Vuong</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
            <a href="https://scholar.google.com/citations?hl=th&user=qyExc4QAAAAJ&view_op=list_works">Minh Nhat Vu</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
            <a href="https://www.linkedin.com/in/hieu-le-trung-7876aa218/">Hieu Le</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=unbPvWAAAAAJ&hl=zh-CN">Baoru Huang</a><sup>3</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=vJYe5lkAAAAJ&hl=en&oi=ao">Binh Huynh</a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </span><br>
          <span class="author-block">
            <a href="https://scholar.google.com.vn/citations?user=UA_83MUAAAAJ&hl=vi">Thieu Vo</a><sup>5</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?hl=en&user=ee4tJYgAAAAJ">Andreas Kugi</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </span>
          <span class="author-block">
            <a href="https://www.csc.liv.ac.uk/~anguyen/">Anh Nguyen</a><sup>6</sup>
          </span>
        </div>
        <br>
        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>1</sup>FPT Software AI Center</span>&nbsp;&nbsp;
          <span class="author-block"><sup>2</sup>ACIN - TU Wien</span>&nbsp;&nbsp;
          <span class="author-block"><sup>3</sup>Imperial College London</span>&nbsp;&nbsp;<br>
          <span class="author-block"><sup>4</sup>HUST</span>&nbsp;&nbsp;
          <span class="author-block"><sup>5</sup>Ton Duc Thang University</span>&nbsp;&nbsp;
          <span class="author-block"><sup>6</sup>University of Liverpool</span>
        </div>
		
        <div class="column has-text-centered">
          <div class="publication-links">
            <span class="link-block">
                <a href="https://arxiv.org/abs/2309.09818" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
            </span>
            <span class="link-block">
              <a href="https://github.com/andvg3/Grasp-Anything"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            <!-- Dataset Link. -->
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-images"></i>
                </span>
                <span>Data (coming soon)</span>
                </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div id="wrapper"> 
        <video id="teaser_4" autoplay controls muted loop playsinline width="100%" height="100%"> 
            <source type="video/mp4" src="./static/videos/Grasp-Anything.mp4" /> 
        </video>
        <div class="clear"></div> 
      </div>
      <h2 class="subtitle has-text-centered">
        <i>Grasp-Anything</i>, a new large-scale language-driven grasp dataset that universally covers objects in our daily lives by using knowledge from foundation models.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Foundation models such as ChatGPT have made significant strides in robotic tasks due to their universal representation of real-world domains. In this paper, we leverage foundation models to tackle grasp detection, a persistent challenge in robotics with broad industrial applications. Despite numerous grasp datasets, their object diversity remains limited compared to real-world figures. Fortunately, foundation models possess an extensive repository of real-world knowledge, including objects we encounter in our daily lives. As a consequence, a promising solution to the limited representation in previous grasp datasets is to harness the universal knowledge embedded in these foundation models. We present Grasp-Anything, a new large-scale grasp dataset synthesized from foundation models to implement this solution. Grasp-Anything excels in diversity and magnitude, boasting 1M samples with text descriptions and more than 3M objects, surpassing prior datasets. Empirically, we show that Grasp-Anything successfully facilitates zero-shot grasp detection on vision-based tasks and real-world robotic experiments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section" id="acknowledgements">
  <div class="container content is-max-desktop">
    <h2 class="title">Acknowledgements</h2>
    <p>We borrow github page from <a href="https://hypernerf.github.io/">HyperNeRF</a>. Special thanks to them!
  </div>
</section>

<script type="text/javascript" src="./static/slick/slick.min.js"></script>
</body>
</html>

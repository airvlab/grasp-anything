---
title: "Grasp-Anything++"
permalink: /docs/grasp-anything-pp/
excerpt: "Overview of Grasp-Anything++."
redirect_from:
  - /theme-setup/
toc: true
---

Grasp-Anything++ extend over Grasp-Anything and includes **10 million** grasping instructions and associated ground truth. 

Our dataset can be used for **language-driven grasping** task and allows the robots to grasp specific objects based on language commands.

## Samples
<p align="center">
  <img src="../../assets/images/grasp-anything-pp-samples.png" alt="samples" style="width: 100%;" />
</p>

## Language-driven Grasping
<p align="center">
  <img src="../../assets/images/grasp-anything-pp-methodology.png" alt="method" style="width: 100%;" />
</p>
We introduce a new language-driven grasping method based on diffusion models with a new contrastive training objective.

## Demonstration
<!--video width="100%" controls>
  <source src="../../assets/images/CVPR24_demo.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video-->

## References
[//]: # [1] Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., & Hoi, S. C. H. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34, 9694-9705.

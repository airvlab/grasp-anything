---
title: "Introduction"
permalink: /docs/introduction/
excerpt: "Large-scale dataset for grasp detection."
redirect_from:
  - /theme-setup/
toc: true
---
Welcome to Grasp-Anything project! We tackle grasp detection by utilizing foundation models. Our project represents a <strong>data centric</strong> approach for grasp detection.

## Dataset Comparison
<p align="center">
  <img src="../../assets/images/intro-dataset-comparison.png" alt="dataset_comparison" style="width: 100%;" />
</p>

Grasp-Anything offers *universality*, featuring a wide range of everyday objects in natural arrangements, unlike other benchmarks limited by object selection and controlled settings.

## Statistics

<p align="center">
  <img src="../../assets/images/homepage-num-samples.png" alt="num-samples" style="width: 50%;" />
  <img src="../../assets/images/homepage-num-objects.png" alt="num-objects" style="width: 49%;" />
</p>

Grasp-Anything significantly outperforms other datasets in terms of number of samples and number of categories.

<p align="center">
  <img src="../../assets/images/pos-tags.png" alt="pos-tags" style="width: 50%;" />
  <img src="../../assets/images/num_cats.png" alt="num-cats" style="width: 48%;" />
</p>

The POS tags in our dataset are visualized in the figure above, highlighting a diverse vocabulary in scene descriptions. By comparing the object shape distributions between Grasp-Anything and Jacquard dataset, Grasp-Anything covers a wider area, suggesting a higher level of shape diversity.

<p align="center">
  <img src="../../assets/images/shape_visualization.png" alt="shape_visualization" style="width: 80%;" />
</p>


## Stay Updated

This website will be continuously updated with the latest papers, datasets, and code. Please check back regularly for updates.

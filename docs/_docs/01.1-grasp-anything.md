---
title: "Grasp-Anything"
permalink: /docs/grasp-anything/
excerpt: "Overview of Grasp-Anything."
redirect_from:
  - /theme-setup/
toc: true
---

Grasp-Anything introduces a new dataset synthesized from foundation models, targeting the challenge of grasp detection in robotics. With **1 million** samples and **3 million** diverse objects, it significantly surpasses previous datasets in *diversity*, facilitating advanced zero-shot and language-driven grasp detection in both simulated and real-world settings.

## Data Pipeline
<p align="center">
  <img src="../../assets/images/generation_pipeline.png" alt="data_pipeline" style="width: 100%;" />
</p>


Our data pipeline begins with prompting ChatGPT with generating an expansive array of scene descriptions in a specific format, aiming for a diverse set of scenarios that include a broad spectrum of objects. Next, we transform these scene descriptions into images using Stable-Diffusion 2.1 [1], and then, with a pretrained RAGT-3/3 model [2], we create plausible grasp poses. These poses are assessed for their viability based on established theories, making our dataset not only vast but of high quality.

## Demonstration
<video width="100%" controls>
  <source src="../../assets/images/ICRA24_0520_VI_fi_compressed.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## Samples
Here are some samples from the Grasp-Anything dataset.
<p align="center">
  <img src="../../assets/images/grasp-anything-samples.png" alt="grasp-anything-sample" style="width: 100%;" />
</p>

## References
[1] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).

[2] Cao, B., Zhou, X., Guo, C., Zhang, B., Liu, Y., & Tan, Q. (2023). NBMOD: Find It and Grasp It in Noisy Background. arXiv preprint arXiv:2306.10265.

